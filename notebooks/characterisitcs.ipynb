{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6eaee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import MobileNetV2ForImageClassification, AutoModelForImageClassification, MobileNetV2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "376278ee-327c-47f9-b1fe-e7fabedf048d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/soft/AIDL/conda_envs/pytorch-200/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at facebook/deit-tiny-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 192]) in the checkpoint and torch.Size([3, 192]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_name  number_of_parameters  inference_speed (ms)\n",
      "0       DEiT               5524995              6.124608\n",
      "1        ViT              85800963             22.955969\n"
     ]
    }
   ],
   "source": [
    "unique_labels = [0, 1, 2]\n",
    "class_names = ['angular_leaf_spot', 'bean_rust', 'healthy']\n",
    "\n",
    "def calculate_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def measure_inference_speed(model, input_data):\n",
    "    with torch.no_grad():\n",
    "        start_time = torch.cuda.Event(enable_timing=True)\n",
    "        end_time = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        start_time.record()\n",
    "        _ = model(input_data)\n",
    "        end_time.record()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        return start_time.elapsed_time(end_time)\n",
    "\n",
    "id2label = {i: name for i, name in zip(unique_labels, class_names)}\n",
    "label2id = {v:k for k, v in id2label.items()}\n",
    "num_classes = len(label2id)\n",
    "num_labels = 3\n",
    "# mobile_net_v2_config = MobileNetV2Config(num_labels=num_labels)\n",
    "# mobile_net_v2_model = MobileNetV2ForImageClassification(mobile_net_v2_config)\n",
    "# mobile_net_v2_model.to('cuda')\n",
    "\n",
    "model_name = \"facebook/deit-tiny-patch16-224\" \n",
    "mobile_net_v2_model = AutoModelForImageClassification.from_pretrained(model_name, num_labels=num_labels, id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True)\n",
    "mobile_net_v2_model.to('cuda')\n",
    "vit_model = AutoModelForImageClassification.from_pretrained(\n",
    "    'merve/beans-vit-224',\n",
    "    num_labels=num_labels,\n",
    "    ignore_mismatched_sizes=True\n",
    ").to('cuda')\n",
    "\n",
    "models = [\n",
    "    (\"DEiT\", mobile_net_v2_model),\n",
    "    (\"ViT\", vit_model), \n",
    "]\n",
    "\n",
    "data = []\n",
    "\n",
    "for model_name, model in models:\n",
    "    num_params = calculate_parameters(model)\n",
    "\n",
    "    input_data = torch.randn(1, 3, 224, 224).to('cuda')  \n",
    "    inference_time = measure_inference_speed(model, input_data)\n",
    "\n",
    "    data.append({\n",
    "        \"model_name\": model_name,\n",
    "        \"number_of_parameters\": num_params,\n",
    "        \"inference_speed (ms)\": inference_time\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc6810b4-3f79-4b33-90df-427fd8c16f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEiT parameters: 5,524,995\n",
      "BERT parameters: 85,800,963\n",
      "Compression ratio (ViT / DEiT): 15.53x\n"
     ]
    }
   ],
   "source": [
    "num_params_mobilenetv2 = calculate_parameters(mobile_net_v2_model)\n",
    "num_params_bert = calculate_parameters(vit_model)\n",
    "\n",
    "compression_ratio = num_params_bert / num_params_mobilenetv2\n",
    "print(f\"DEiT parameters: {num_params_mobilenetv2:,}\")\n",
    "print(f\"BERT parameters: {num_params_bert:,}\")\n",
    "print(f\"Compression ratio (ViT / DEiT): {compression_ratio:.2f}x\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch 2.0.0",
   "language": "python",
   "name": "torch200"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
